import pandas as pd
import h5py
from io import StringIO
import numpy as np
import glob

# def h5_to_df(
#     filename,
#     scd_stats=["SCD", "SSD"] + ["alt_INS-16", "alt_INS-32", "alt_INS-64", "alt_INS-128", "alt_INS-256"] + ["ref_INS-16", "ref_INS-32", "ref_INS-64", "ref_INS-128", "ref_INS-256"],
#     drop_duplicates_key="index",
#     verbose=True,
# ):

#     """
#     Load an h5 file, as generated by scripts in akita_utils/bin/
#     as a DataFrame for analysis. Currently this function expects
#     that keys with statistics are (N x #model_outputs) matrices
#     and adds an average across all model outputs as a dataFrame column.

#     Parameters
#     ----------
#     filename : str
#         Name of h5 file to be loaded.

#     scd_stats : list of str
#         Names of keys storing computed statistics.

#     drop_duplicates : str or None
#         Attempts to drop duplicated rows based on provided str.
#         If None, no dupliate removal attempted.

#     Returns
#     -------
#     df_out : pd.DataFrame

#     """

#     hf = h5py.File(filename, "r")
#     df_out = pd.DataFrame()

#     for stat in scd_stats:
#         keys = [key for key in hf.keys() if stat in key and key not in scd_stats]
#         if keys:
#             data = pd.DataFrame()
#             for key in keys:
#                 if verbose:
#                     print(f"varying target, {key}")
#                 series = pd.Series(hf[key][()], name=key)
#                 data = pd.concat([data, series], axis=1)

#             if not data.empty: 
#                 average = data.mean(axis=1)
#                 df_out = pd.concat([df_out, pd.Series(average, name=stat)],axis=1)

#     remaining_keys = [key for key in hf.keys() if all(stat not in key for stat in scd_stats)]
#     exact_matches = [key for key in hf.keys() if key in scd_stats]
    
#     for key in remaining_keys:
#         if verbose:
#             print(f"remaining_keys, {key}")
#         df_out = pd.concat([df_out, pd.Series(hf[key][()], name=key)],axis=1)
        
#     for key in exact_matches:
#         if verbose:
#             print(f"exact match thus using old pipeline processing for {key}")
#         df_out = pd.concat([df_out, pd.Series(pd.DataFrame(hf[key][()]).mean(axis=1), name=key)], axis=1)             
#     hf.close()

#     # adding difference between reference and alternate insulation
#     insulation_stats = ["INS-16", "INS-32", "INS-64", "INS-128", "INS-256"]
#     for key in insulation_stats:
#         if "ref_" + key in df_out.columns:
#             df_out[key] = df_out["alt_" + key] - df_out["ref_" + key]
            
#     for key, key_dtype in df_out.dtypes.items():
#         if not pd.api.types.is_numeric_dtype(key_dtype):
#             df_out[key] = df_out[key].str.decode("utf8").copy()

#     if drop_duplicates_key is not None:
#         len_orig = len(df_out)
#         if drop_duplicates_key not in df_out.keys():
#             raise ValueError("duplicate removal key must be present in dataFrame")
#         df_out.drop_duplicates(drop_duplicates_key, inplace=True)
#         if verbose:
#             print(len_orig - len(df_out), "duplicates removed for ", filename)
#         df_out.reset_index(inplace=True, drop=True)

#     return df_out


def h5_to_df(filename, drop_duplicates_key="index", verbose=False):
    """
    Load an HDF5 file, as generated by scripts in akita_utils/bin/,
    as a DataFrame for analysis.
    
    Parameters:
    filename (str): Name of the HDF5 file to be loaded.
    drop_duplicates_key (str or None): Key used to identify duplicate rows for removal. If None, no duplicate removal is performed.
    verbose (bool): Whether to print the number of duplicates removed.
    
    Returns:
    df_out (pd.DataFrame): Loaded DataFrame.
    """
    hf = h5py.File(filename, "r")
    df_out = pd.DataFrame()
    
    
    for key in hf.keys():
        series = pd.Series(hf[key][()], name=key)
        df_out = pd.concat([df_out, series],axis=1)

    for key, key_dtype in df_out.dtypes.items():
        if not pd.api.types.is_numeric_dtype(key_dtype):
            df_out[key] = df_out[key].str.decode("utf8")

    if drop_duplicates_key is not None:
        len_orig = len(df_out)
        if drop_duplicates_key not in df_out.columns:
            raise ValueError("Duplicate removal key must be present in DataFrame")
        df_out.drop_duplicates(drop_duplicates_key, inplace=True)
        if verbose:
            print(len_orig - len(df_out), "duplicates removed for", filename)
        df_out.reset_index(drop=True, inplace=True)

    return df_out


def multi_h5_to_df(h5_dirs):
    """
    Load multiple HDF5 files and concatenate them into a single DataFrame.

    Parameters:
    h5_dirs (str): Directory path or glob pattern to select HDF5 files.

    Returns:
    dfs (pd.DataFrame): Concatenated DataFrame containing data from all HDF5 files.
    """
    dfs = pd.DataFrame()
    for h5_file in glob.glob(h5_dirs):
        df = h5_to_df(h5_file, drop_duplicates_key=None)
        dfs = pd.concat([dfs, df], axis=1)
    return dfs


def calculate_mean_stats_across_targets(df, stats, models, heads):
    """
    Calculate mean statistics across target columns based on specified models and heads.

    Parameters:
    df (pd.DataFrame): Input DataFrame.
    stats (list): List of statistics to calculate mean for.
    models (list): List of model numbers.
    heads (list): List of head numbers. [0,1] 0=human, 1=mouse

    Returns:
    new_df (pd.DataFrame): DataFrame with added columns for mean statistics.
    """
    new_df = df.copy()
    for head in heads:
        for model in models:
            for stat in stats:
                new_col_name = f"{stat}_h{head}_m{model}_targets_mean"
                filtered_cols = [col for col in df.columns if f"{stat}_h{head}_m{model}_" in col]
                if not df[filtered_cols].empty:
                    new_df[new_col_name] = df[filtered_cols].mean(axis=1)
                else:
                    print(f"No data for {stat}_h{head}_m{model} targets")
    return new_df


def calculate_mean_stats_across_models(df, stats, heads):
    """
    Calculate mean statistics across models based on specified heads.

    Parameters:
    df (pd.DataFrame): Input DataFrame.
    stats (list): List of statistics to calculate mean for.
    heads (list): List of head numbers.[0,1] 0=human, 1=mouse

    Returns:
    new_df (pd.DataFrame): DataFrame with added columns for mean statistics.
    """
    new_df = df.copy()
    for head in heads:
        for stat in stats:
            new_col_name = f"{stat}" if len(heads) == 1 else f"{stat}_h{head}"
            filtered_cols = [col for col in df.columns if f"{stat}_h{head}" in col and "targets_mean" not in col]            
            new_df[new_col_name] = df[filtered_cols].mean(axis=1)
    return new_df

def read_jaspar_to_numpy(
    motif_file="/project/fudenber_735/motifs/pfms/JASPAR2022_CORE_redundant_pfms_jaspar/MA0139.1.jaspar",
    normalize=True,
):
    """
    Read a jaspar pfm to a numpy array that can be used with scan_motif. Default motif is CTCF (MA0139.1)

    Parameters
    ----------
    motif_file : str
        Default CTCF motif file.
    normalize :
        Whether to normalize counts to sum to one for each position in the motif. Default True.

    Returns
    -------
    motif : np.array
        n_positions by 4 bases

    """

    with open(motif_file, "r") as f:
        motif = []
        for line in f.readlines():
            if ">" in line:
                continue
            else:
                motif.append(line.strip().replace("[", "").replace("]", "").split())
    motif = pd.DataFrame(motif).set_index(0).astype(float).values.T
    if normalize is True:
        motif /= motif.sum(axis=1)[:, None]
    if motif.shape[1] != 4:
        raise ValueError("motif returned should be have n_positions x 4 bases")
    return motif


def read_rmsk(rmsk_file="/project/fudenber_735/genomes/mm10/database/rmsk.txt.gz"):
    
    """reads a data frame containing repeatable elements and renames columns specifying genomic intervals to standard: chrom, start, end, used in thie repo."""
    
    rmsk_cols = list(
        pd.read_csv(
            StringIO(
                """bin swScore milliDiv milliDel milliIns genoName genoStart genoEnd genoLeft strand repName repClass repFamily repStart repEnd repLeft id"""
            ),
            sep=" ",
        )
    )

    rmsk = pd.read_table(
        rmsk_file,
        names=rmsk_cols,
    )
    
    rmsk.rename(
        columns={"genoName": "chrom", "genoStart": "start", "genoEnd": "end"},
        inplace=True,
    )
    
    return rmsk

def read_ctcf(ctcf_file="/project/fudenber_735/motifs/mm10/jaspar/MA0139.1.tsv.gz"):
    ctcf_cols = list(pd.read_csv(StringIO("""chrom start end name score pval strand"""), sep=" ",))

    ctcf_motifs = pd.read_table(ctcf_file, names=ctcf_cols,)
    
    return ctcf_motifs
