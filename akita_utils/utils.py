### akita utilities

import bioframe
import pandas as pd
import numpy as np
import tensorflow as tf
<<<<<<< HEAD
from basenji import dna_io
from io import StringIO
import pysam
import time 


### numeric utilites

from scipy.stats import spearmanr, pearsonr


=======
import glob
from io import StringIO
import h5py
import random


### numeric utilites
>>>>>>> main
def absmaxND(a, axis=None):
    """
    https://stackoverflow.com/a/39152275
    """
    amax = a.max(axis)
    amin = a.min(axis)
    return np.where(-amin > amax, amin, amax)


<<<<<<< HEAD
import scipy.signal


=======
>>>>>>> main
def smooth(y, box_pts):
    box = np.ones(box_pts) / box_pts
    y_smooth = np.convolve(y, box, mode="same")
    return y_smooth


<<<<<<< HEAD
def set_diag(arr, x, i=0, copy=False):
    if copy:
        arr = arr.copy()
    start = max(i, -arr.shape[1] * i)
    stop = max(0, (arr.shape[1] - i)) * arr.shape[1]
    step = arr.shape[1] + 1
    arr.flat[start:stop:step] = x
    return arr


### model i/o
def from_upper_triu(vector_repr, matrix_len, num_diags):
    z = np.zeros((matrix_len, matrix_len))
    triu_tup = np.triu_indices(matrix_len, num_diags)
    z[triu_tup] = vector_repr
    for i in range(-num_diags + 1, num_diags):
        set_diag(z, np.nan, i)
    return z + z.T
=======
def ut_dense(preds_ut, diagonal_offset):
    """Construct symmetric dense prediction matrices from upper triangular vectors.

    Parameters
    -----------
    preds_ut : ( M x O) numpy array
        Upper triangular matrix to convert. M is the number of upper triangular entries,
        and O corresponds to the number of different targets.
    diagonal_offset : int
        Number of diagonals that are added as zeros in the conversion.
        Typically 2 diagonals are ignored in Hi-C data processing.

    Returns
    --------
    preds_dense : (D x D x O) numpy array
        Each output upper-triangular vector is converted to a symmetric D x D matrix.
        Output matrices have zeros at the diagonal for `diagonal_offset` number of diagonals.

    """
    ut_len, num_targets = preds_ut.shape

    # infer original sequence length
    seq_len = int(np.sqrt(2 * ut_len + 0.25) - 0.5)
    seq_len += diagonal_offset

    # get triu indexes
    ut_indexes = np.triu_indices(seq_len, diagonal_offset)
    assert len(ut_indexes[0]) == ut_len

    # assign to dense matrix
    preds_dense = np.zeros(shape=(seq_len, seq_len, num_targets), dtype=preds_ut.dtype)
    preds_dense[ut_indexes] = preds_ut

    # symmetrize
    preds_dense += np.transpose(preds_dense, axes=[1, 0, 2])

    return preds_dense
>>>>>>> main


### score i/o


<<<<<<< HEAD
def h5_to_df(filename):
    scd_out = h5py.File(filename, "r")
    s = []
    scd_stats = ["SCD", "SSD", "INS-16", "INS-32", "INS-64", "INS-128", "INS-256"]
    for key in scd_out.keys():
        if key.replace("ref_", "").replace("alt_", "") in scd_stats:
            s.append(pd.Series(scd_out[key][()].mean(axis=1), name=key))
        else:
            s.append(pd.Series(scd_out[key][()], name=key))
            # print(len(scd_out[key][()]))

    ins_stats = ["INS-16", "INS-32", "INS-64", "INS-128", "INS-256"]
    for key in ins_stats:
        if "ref_" + key in scd_out.keys():
            diff = scd_out["ref_" + key][()].mean(axis=1) - scd_out["alt_" + key][
                ()
            ].mean(axis=1)
            s.append(pd.Series(diff, name=key))
    seq_coords_df = pd.concat(s, axis=1)
    for key in ["chrom", "strand_2"]:  #'rownames','strand','chrom','TF']:
        seq_coords_df[key] = seq_coords_df[key].str.decode("utf8").copy()
    scd_out.close()

    len_orig = len(seq_coords_df)
    seq_coords_df.drop_duplicates("index", inplace=True)
    print(len_orig - len(seq_coords_df), "duplicates removed for ", filename)
    seq_coords_df.rename(columns={"index": "mut_index"}, inplace=True)
    seq_coords_df.reset_index(inplace=True, drop=True)
    return seq_coords_df


import glob
from io import StringIO


def filter_boundary_h5(
    h5_dirs="/project/fudenber_735/tensorflow_models/akita/v2/analysis/permute_boundaries_motifs_ctcf_mm10_model*/scd.h5",
    score_key="SCD",
=======
def h5_to_df(
    filename,
    scd_stats=["SCD", "SSD", "INS"],
    drop_duplicates_key="index",
    verbose=False,
):

    """
    Load an h5 file, as generated by scripts in akita_utils/bin/
    as a DataFrame for analysis. Currently this function expects
    that keys with statistics are (N x #model_outputs) matrices
    and adds an average across all model outputs as a dataFrame column.

    Parameters
    ----------
    filename : str
        Name of h5 file to be loaded.

    scd_stats : list of str
        Names of keys storing computed statistics.

    drop_duplicates : str or None
        Attempts to drop duplicated rows based on provided str.
        If None, no dupliate removal attempted.

    Returns
    -------
    df_out : pd.DataFrame

    """

    hf = h5py.File(filename, "r")
    s = []
    for key in hf.keys():
        if key.replace("ref_", "").replace("alt_", "").split("-")[0] in scd_stats:
            if verbose:
                print(key)
            s.append(pd.Series(hf[key][()].mean(axis=1), name=key))
        else:
            s.append(pd.Series(hf[key][()], name=key))

    # adding difference between reference and alternate insulation
    insulation_stats = ["INS-16", "INS-32", "INS-64", "INS-128", "INS-256"]
    for key in insulation_stats:
        if "ref_" + key in hf.keys():
            diff = hf["ref_" + key][()].mean(axis=1) - hf["alt_" + key][()].mean(axis=1)
            s.append(pd.Series(diff, name=key))
    hf.close()

    # generating pandas DataFrame and converting bytestrings
    df_out = pd.concat(s, axis=1)
    for key, key_dtype in df_out.dtypes.items():
        if not pd.api.types.is_numeric_dtype(key_dtype):
            df_out[key] = df_out[key].str.decode("utf8").copy()

    if drop_duplicates_key is not None:
        len_orig = len(df_out)
        if drop_duplicates_key not in df_out.keys():
            raise ValueError("duplicate removal key must be present in dataFrame")
        df_out.drop_duplicates(drop_duplicates_key, inplace=True)
        if verbose:
            print(len_orig - len(df_out), "duplicates removed for ", filename)
        df_out.reset_index(inplace=True, drop=True)

    return df_out


def _split_spans(sites, concat=False, span_cols=["start_2", "end_2"]):
    """Helper function to split a span 'start-end' into two integer series, and either
    return as a dataFrame or concatenate to the input dataFrame"""

    sites_spans_split = (
        sites["span"]
        .str.split("-", expand=True)
        .astype(int)
        .rename(columns={0: span_cols[0], 1: span_cols[1]})
        .copy()
    )
    if concat:
        return pd.concat(
            [sites, sites_spans_split],
            axis=1,
        )

    else:
        return sites_spans_split


def filter_boundary_ctcfs_from_h5(
    h5_dirs="/project/fudenber_735/tensorflow_models/akita/v2/analysis/permute_boundaries_motifs_ctcf_mm10_model*/scd.h5",
    score_key="SCD",
    threshold_all_ctcf=5,
>>>>>>> main
):
    """Takes a set of boundary mutagenesis dataframes as input, where individual sites are saved in the 'span' column,
    extracts sites greater than a threshold, and filters out sites that overlap with repeatmasker elements.
    """
    ## load scores from boundary mutagenesis, average chosen score across models
    dfs = []
    for h5_file in glob.glob(h5_dirs):
        dfs.append(h5_to_df(h5_file))
<<<<<<< HEAD
    score_key = "SCD"
    df = dfs[0].copy()
    df[score_key] = np.mean([df[score_key] for df in dfs], axis=0)
    df["span"] = df["span"].str.decode("utf8")

    ## append scores for full mut and all ctcf mut to table
=======
    df = dfs[0].copy()
    df[score_key] = np.mean([df[score_key] for df in dfs], axis=0)

    # append scores for full mut and all ctcf mut to table
>>>>>>> main
    print("annotating each site with boundary-wide scores")
    score_10k = np.zeros((len(df),))
    score_all_ctcf = np.zeros((len(df),))
    for i in np.unique(df["boundary_index"].values):
        inds = df["boundary_index"].values == i
        df_boundary = df.iloc[inds]
<<<<<<< HEAD
        score_10k[inds] = df_boundary.iloc[-1]["SCD"]
        if len(df_boundary) > 2:
            score_all_ctcf[inds] = df_boundary.iloc[-2]["SCD"]
=======
        score_10k[inds] = df_boundary.iloc[-1][score_key]
        if len(df_boundary) > 2:
            score_all_ctcf[inds] = df_boundary.iloc[-2][score_key]
>>>>>>> main
    df["score_all_ctcf"] = score_all_ctcf
    df["score_10k"] = score_10k

    # considering only single ctcf mutations
    # require that they fall in an overall boundary that has some saliency
    # TODO: maybe also require that the neighboring bins don't have a more salient boundary?
    # suffix _2 means _motif
    sites = df.iloc[
<<<<<<< HEAD
        (df["strand_2"].values != "nan") * (df["score_all_ctcf"].values > 5)
    ].copy()

    # extracting start/end of motif from span
    sites = pd.concat(
        [
            sites,
            sites["span"]
            .str.split("-", expand=True)
            .astype(int)
            .rename(columns={0: "start_2", 1: "end_2"})
            .copy(),
        ],
        axis=1,
    )
    sites.reset_index(inplace=True, drop=True)

    print("filtering sites by overlap with rmsk")
    # require that sites don't overlap rmsk !
    # this is important for sineB2 in mice, maybe other things as well
    rmsk_cols = pd.read_csv(
        StringIO(
            "bin	swScore	milliDiv	milliDel	milliIns	genoName	genoStart	genoEnd	genoLeft	strand	repName	repClass	repFamily	repStart	repEnd	repLeft	id"
        ),
        sep="\t",
    )
    rmsk = pd.read_table(
        "/project/fudenber_735/genomes/mm10/database/rmsk.txt.gz",
        names=rmsk_cols.keys(),
=======
        (df["strand_2"].values != "nan")
        * (df["score_all_ctcf"].values > threshold_all_ctcf)
    ].copy()

    # extracting start/end of motif from span
    sites = _split_spans(sites, concat=True)
    sites.reset_index(inplace=True, drop=True)
    if sites.duplicated().sum() > 0:
        raise ValueError("no duplicates allowed")
    return sites


def filter_by_chrmlen(df, chrmsizes, buffer_bp=0):
    """
    filter a dataFrame of intervals by a such than none exceed supplied chromosome
    sizes.

    Parameters
    ------------
    df : dataFrame
        Input dataframe
    chrmsizes : chrmsizes file or dictionary that can be converted to a view
        Input chromosome sizes for filtering
    buffer_bp : int
        Size of zone to exclude intervals at chrom starts or ends.

    Returns
    ---------
    df_filtered : dataFrame
        Subset of intervals that do not exceed chromosome size when extended
    """
    assert type(buffer_bp) is int
    if (type(chrmsizes) is not dict) and (
        type(chrmsizes) is not pd.core.frame.DataFrame
    ):
        chrmsizes = bioframe.read_chromsizes(chrmsizes)
    view_df = bioframe.from_any(chrmsizes)
    chromend_zones = view_df.copy()
    chromend_zones["start"] = chromend_zones["end"] - buffer_bp
    chromstart_zones = view_df.copy()
    chromstart_zones["end"] = chromstart_zones["start"] + buffer_bp
    filter_zones = pd.concat([chromend_zones, chromstart_zones]).reset_index(drop=True)
    df_filtered = bioframe.setdiff(df, filter_zones)
    return df_filtered


def filter_by_rmsk(
    sites,
    rmsk_file="/project/fudenber_735/genomes/mm10/database/rmsk.txt.gz",
    verbose=True,
):
    """
    Filter out sites that overlap any entry in rmsk.
    This is important for sineB2 in mice, and perhaps for other repetitive elements as well.

    Parameters
    -----------
    sites : dataFrame
        Set of genomic intervals, currently with columns "chrom","start_2","end_2"
        TODO: update this and allow columns to be passed
    rmsk_file : str
        File in repeatmasker format used for filtering sites.

    Returns
    --------
    sites : dataFrame
        Subset of sites that do not have overlaps with repeats in the rmsk_file.

    """
    if verbose:
        print("filtering sites by overlap with rmsk")

    rmsk_cols = list(
        pd.read_csv(
            StringIO(
                """bin swScore milliDiv milliDel milliIns genoName genoStart genoEnd genoLeft strand repName repClass repFamily repStart repEnd repLeft id"""
            ),
            sep=" ",
        )
    )

    rmsk = pd.read_table(
        rmsk_file,
        names=rmsk_cols,
>>>>>>> main
    )
    rmsk.rename(
        columns={"genoName": "chrom", "genoStart": "start", "genoEnd": "end"},
        inplace=True,
    )

    sites = bioframe.count_overlaps(
        sites, rmsk[["chrom", "start", "end"]], cols1=["chrom", "start_2", "end_2"]
    )
    sites = sites.iloc[sites["count"].values == 0]
    sites.reset_index(inplace=True, drop=True)
<<<<<<< HEAD
    if sites.duplicated().sum() > 0:
        raise ValueError("no duplicates allowed")
=======
>>>>>>> main

    return sites


def filter_sites_by_score(
    sites,
    score_key="SCD",
    weak_thresh_pct=1,  # don't use sites weaker than this, might be artifacts
    weak_num=500,
<<<<<<< HEAD
    strong_thresh_pct=99,  # don't use sites weaker than this, might be artifacts
    strong_num=500,
):
    """chooses a specified number of strong and weak sites exluding low and/or high outliers which may contain more artifacts."""
=======
    strong_thresh_pct=99,  # don't use sites stronger than this, might be artifacts
    strong_num=500,
):
    """Chooses a specified number of strong and weak sites exluding low and/or high outliers which may contain more artifacts."""
>>>>>>> main
    if (weak_num < 1) or (strong_num < 1):
        raise ValueError("must select a postive number of sites")
    strong_thresh = np.percentile(sites[score_key].values, strong_thresh_pct)
    weak_thresh = np.percentile(sites[score_key].values, weak_thresh_pct)
    weak_sites = (
        sites.loc[sites[score_key] > weak_thresh]
        .copy()
        .sort_values(score_key)[:weak_num]
    )
    strong_sites = (
        sites.loc[sites[score_key] < strong_thresh]
        .copy()
        .sort_values(score_key)[-strong_num:][::-1]
    )
    strong_sites.reset_index(inplace=True, drop=True)
    weak_sites.reset_index(inplace=True, drop=True)
    return strong_sites, weak_sites


def prepare_insertion_tsv(
    h5_dirs="/project/fudenber_735/tensorflow_models/akita/v2/analysis/permute_boundaries_motifs_ctcf_mm10_model*/scd.h5",
    score_key="SCD",
<<<<<<< HEAD
    flank_pad=60,  # how much flanking sequence around the sites to include
    weak_thresh_pct=1,  # don't use sites weaker than this, might be artifacts
    weak_num=500,
    strong_thresh_pct=99,  # don't use sites weaker than this, might be artifacts
=======
    threshold_all_ctcf=5,  # reasonable cutoff for ctcf sites with impactful genomic SCD
    pad_flank=60,  # how much flanking sequence around the sites to include
    weak_thresh_pct=1,  # don't use sites weaker than this, might be artifacts
    weak_num=500,
    strong_thresh_pct=99,  # don't use sites stronger than this, might be artifacts
>>>>>>> main
    strong_num=500,
    save_tsv=None,  # optional filename to save a tsv
):
    """creates a tsv with strong followed by weak sequences, which can be used as input to akita_insert.py or akita_flat_map.py"""

<<<<<<< HEAD
    sites = filter_boundary_h5(h5_dirs=h5_dirs, score_key=score_key)
=======
    sites = filter_boundary_ctcfs_from_h5(
        h5_dirs=h5_dirs, score_key=score_key, threshold_all_ctcf=threshold_all_ctcf
    )
>>>>>>> main

    strong_sites, weak_sites = filter_sites_by_score(
        sites,
        score_key=score_key,
        weak_thresh_pct=weak_thresh_pct,
        weak_num=weak_num,
        strong_thresh_pct=strong_thresh_pct,
        strong_num=strong_num,
    )

    site_df = pd.concat([strong_sites.copy(), weak_sites.copy()])
    seq_coords_df = (
<<<<<<< HEAD
        site_df[["chrom", "start_2", "end_2", "strand_2", "SCD"]]
=======
        site_df[["chrom", "start_2", "end_2", "strand_2", score_key]]
>>>>>>> main
        .copy()
        .rename(
            columns={
                "start_2": "start",
                "end_2": "end",
                "strand_2": "strand",
<<<<<<< HEAD
                "SCD": "genomic_SCD",
            }
        )
    )
    seq_coords_df.reset_index(inplace=True)
    seq_coords_df = bioframe.expand(seq_coords_df, pad=flank_pad)
=======
                score_key: "genomic_" + score_key,
            }
        )
    )
    seq_coords_df.reset_index(drop=True, inplace=True)
    seq_coords_df.reset_index(inplace=True)
    seq_coords_df = bioframe.expand(seq_coords_df, pad=pad_flank)
>>>>>>> main
    print("df prepared")
    if save_tsv is not None:
        seq_coords_df.to_csv(save_tsv, sep="\t", index=False)
    return seq_coords_df


def _generate_paired_core_flank_df(site_df, pad_core=0, pad_flank=60):
    pair_columns = (
        list(site_df.columns + "_core")
        + ["pad_core"]
        + list(site_df.columns + "_flank")
        + ["pad_flank"]
    )
    all_pairs = []
    for core_ind, core_site in site_df.copy().iterrows():
        for flank_ind, flank_site in site_df.copy().iterrows():
            all_pairs.append(
                list(core_site.values)
                + list([pad_core])
                + list(flank_site.values)
                + list([pad_flank])
            )
    return pd.DataFrame(all_pairs, columns=pair_columns)


def prepare_paired_insertion_df(
    h5_dirs="/project/fudenber_735/tensorflow_models/akita/v2/analysis/permute_boundaries_motifs_ctcf_mm10_model*/scd.h5",
    score_key="SCD",
    pad_core=0,
    pad_flank=60,
    weak_thresh_pct=1,  # don't use sites weaker than this, might be artifacts
    weak_num=500,
    strong_thresh_pct=99,  # don't use sites stronger than this, might be artifacts
    strong_num=500,
    save_tsv=None,  # optional filename to save a tsv
):
    """
    Prepare a DataFrame for paired motif insertions into a background sequence.

    Parameters
    ----------
    As for prepare_insertion_tsv, but with the addition of pad_core and pad_flank.

<<<<<<< HEAD

def dna_rc(seq):
    return seq.translate(str.maketrans("ATCGatcg", "TAGCtagc"))[::-1]


def permute_seq_k(seq_1hot, k=2):
=======
    Returns
    -------
    df_site_pairs : pd.DataFrame
        14-column dataframe with:
        index_core, chrom_core, start_core, end_core, strand_core score_key_core, pad_core,
        index_core, chrom_flank, start_flank, end_flank, strand_flank, score_key_flank, pad_flank

    """
    if pad_core > pad_flank:
        raise ValueError("the flanking sequence must be longer than the core sequence")
    site_df = prepare_insertion_tsv(
        h5_dirs=h5_dirs,
        score_key=score_key,
        weak_thresh_pct=weak_thresh_pct,
        weak_num=weak_num,
        strong_num=strong_num,
        strong_thresh_pct=strong_thresh_pct,
        save_tsv=None,
        pad_flank=0,
    )
    df_site_pairs = _generate_paired_core_flank_df(
        site_df, pad_core=pad_core, pad_flank=pad_flank
    )

    print("paired df prepared")
    if save_tsv is not None:
        df_site_pairs.to_csv(save_tsv, sep="\t", index=False)
    return df_site_pairs


# sequence handling
def dna_rc(seq):
    return seq.translate(str.maketrans("ATCGatcg", "TAGCtagc"))[::-1]

def dna_1hot_index(seq, n_sample=False):
  """ dna_1hot_index
    Args:
      seq:       nucleotide sequence.
    Returns:
      seq_code:  index int array representation.
    """
  seq_len = len(seq)
  seq = seq.upper()

  # map nt's to a len(seq) of 0,1,2,3
  seq_code = np.zeros(seq_len, dtype='uint8')
    
  for i in range(seq_len):
    nt = seq[i]
    if nt == 'A':
      seq_code[i] = 0
    elif nt == 'C':
      seq_code[i] = 1
    elif nt == 'G':
      seq_code[i] = 2
    elif nt == 'T':
      seq_code[i] = 3
    else:
      if n_sample:
        seq_code[i] = random.randint(0,3)
      else:
        seq_code[i] = 4

  return seq_code

def dna_1hot_GC(seq):
  seq_len = len(seq)
  seq = seq.upper()
  # map nt's to a len(seq) of 0 = A,T; 1 = G,C
  seq_code = np.zeros(seq_len, dtype='uint8')
  for i in range(seq_len):
    nt = seq[i]
    if nt == 'A':
      seq_code[i] = 0
    elif nt == 'C':
      seq_code[i] = 1
    elif nt == 'G':
      seq_code[i] = 1
    elif nt == 'T':
      seq_code[i] = 0
    else:
      seq_code[i] = random.randint(0,1)         
  return seq_code


def dna_1hot(seq, seq_len=None, n_uniform=False, n_sample=False):
  """ dna_1hot
    Args:
      seq:       nucleotide sequence.
      seq_len:   length to extend/trim sequences to.
      n_uniform: represent N's as 0.25, forcing float16,
      n_sample:  sample ACGT for N
    Returns:
      seq_code: length by nucleotides array representation.
    """
  if seq_len is None:
    seq_len = len(seq)
    seq_start = 0
  else:
    if seq_len <= len(seq):
      # trim the sequence
      seq_trim = (len(seq) - seq_len) // 2
      seq = seq[seq_trim:seq_trim + seq_len]
      seq_start = 0
    else:
      seq_start = (seq_len - len(seq)) // 2

  seq = seq.upper()

  # map nt's to a matrix len(seq)x4 of 0's and 1's.
  if n_uniform:
    seq_code = np.zeros((seq_len, 4), dtype='float16')
  else:
    seq_code = np.zeros((seq_len, 4), dtype='bool')
    
  for i in range(seq_len):
    if i >= seq_start and i - seq_start < len(seq):
      nt = seq[i - seq_start]
      if nt == 'A':
        seq_code[i, 0] = 1
      elif nt == 'C':
        seq_code[i, 1] = 1
      elif nt == 'G':
        seq_code[i, 2] = 1
      elif nt == 'T':
        seq_code[i, 3] = 1
      else:
        if n_uniform:
          seq_code[i, :] = 0.25
        elif n_sample:
          ni = random.randint(0,3)
          seq_code[i, ni] = 1

  return seq_code




def permute_seq_k(seq_1hot, k=2):
    """
    Permute a 1hot encoded sequence by k-mers.

    Parameters
    ----------
    seq_1hot : numpy.array
        n_bases x 4 array
    k : int
        number of bases kept together in permutations.
    """

>>>>>>> main
    if np.mod(k, 2) != 0:
        raise ValueError("current implementation only works for multiples of 2")
    seq_1hot_perm = np.zeros(np.shape(seq_1hot)).astype(int)
    perm_inds = k * np.random.permutation(np.arange(len(seq_1hot) // k))
    for i in range(k):
        seq_1hot_perm[i::k] = seq_1hot[perm_inds + i, :].copy()
    return seq_1hot_perm


<<<<<<< HEAD
### motif handling
=======
# motif handling


def hot1_rc(seqs_1hot):
    """Reverse complement a batch of one hot coded sequences,
    while being robust to additional tracks beyond the four
    nucleotides."""

    if seqs_1hot.ndim == 2:
        singleton = True
        seqs_1hot = np.expand_dims(seqs_1hot, axis=0)
    else:
        singleton = False

    seqs_1hot_rc = seqs_1hot.copy()

    # reverse
    seqs_1hot_rc = seqs_1hot_rc[:, ::-1, :]

    # swap A and T
    seqs_1hot_rc[:, :, [0, 3]] = seqs_1hot_rc[:, :, [3, 0]]

    # swap C and G
    seqs_1hot_rc[:, :, [1, 2]] = seqs_1hot_rc[:, :, [2, 1]]

    if singleton:
        seqs_1hot_rc = seqs_1hot_rc[0]

    return seqs_1hot_rc


>>>>>>> main
def scan_motif(seq_1hot, motif, strand=None):
    if motif.shape[-1] != 4:
        raise ValueError("motif should be n_postions x 4 bases, A=0, C=1, G=2, T=3")
    if seq_1hot.shape[-1] != 4:
        raise ValueError("seq_1hot should be n_postions x 4 bases, A=0, C=1, G=2, T=3")
    scan_forward = tf.nn.conv1d(
        np.expand_dims(seq_1hot, 0).astype(float),
        np.expand_dims(motif, -1).astype(float),
        stride=1,
        padding="SAME",
    ).numpy()[0]
    if strand == "forward":
        return scan_forward
    scan_reverse = tf.nn.conv1d(
        np.expand_dims(seq_1hot, 0).astype(float),
<<<<<<< HEAD
        np.expand_dims(dna_io.hot1_rc(motif), -1).astype(float),
=======
        np.expand_dims(hot1_rc(motif), -1).astype(float),
>>>>>>> main
        stride=1,
        padding="SAME",
    ).numpy()[0]
    if strand == "reverse":
        return scan_reverse
    return np.maximum(scan_forward, scan_reverse).flatten()

<<<<<<< HEAD

def read_jaspar_to_numpy(motif_file, normalize=True):
    ## read jaspar pfm
=======

def read_jaspar_to_numpy(
    motif_file="/project/fudenber_735/motifs/pfms/JASPAR2022_CORE_redundant_pfms_jaspar/MA0139.1.jaspar",
    normalize=True,
):
    """
    Read a jaspar pfm to a numpy array that can be used with scan_motif. Default motif is CTCF (MA0139.1)

    Parameters
    ----------
    motif_file : str
        Default CTCF motif file.
    normalize :
        Whether to normalize counts to sum to one for each position in the motif. Default True.

    Returns
    -------
    motif : np.array
        n_positions by 4 bases

    """

>>>>>>> main
    with open(motif_file, "r") as f:
        motif = []
        for line in f.readlines():
            if ">" in line:
                continue
            else:
                motif.append(line.strip().replace("[", "").replace("]", "").split())
    motif = pd.DataFrame(motif).set_index(0).astype(float).values.T
<<<<<<< HEAD
    if normalize == True:
=======
    if normalize is True:
>>>>>>> main
        motif /= motif.sum(axis=1)[:, None]
    if motif.shape[1] != 4:
        raise ValueError("motif returned should be have n_positions x 4 bases")
    return motif


<<<<<<< HEAD
def create_flat_seqs(
    seqnn_model,
    genome_fasta,
    seq_length,
    dataframe,
    max_iters=1,
    batch_size=6,
    shuffle_k=8,
    ctcf_thresh=8,
    scores_thresh=5500,
    scores_pixelwise_thresh=0.04,
    masking = False
):
    """
    This function creates flat sequences
    """
    flat_seqs = []
    num_seqs = dataframe.shape[0]

    mot = ">CCAsyAGrkGGCr\n0.0000\t1.0000\t0.0000\t0.0000\n0.0000\t1.0000\t0.0000\t0.0000\n1.0000\t0.0000\t0.0000\t0.0000\n0.0000\t0.5000\t0.5000\t0.0000\n0.0000\t0.5000\t0.0000\t0.5000\n1.0000\t0.0000\t0.0000\t0.0000\n0.0000\t0.0000\t1.0000\t0.0000\n0.5000\t0.0000\t0.5000\t0.0000\n0.0000\t0.0000\t0.5000\t0.5000\n0.0000\t0.0000\t1.0000\t0.0000\n0.0000\t0.0000\t1.0000\t0.0000\n0.0000\t1.0000\t0.0000\t0.0000\n0.5000\t0.0000\t0.5000\t0.0000"
    motif = pd.read_csv(
        StringIO(mot), sep="\t", header=0, names=["A", "C", "G", "T"]
    ).values
    motif_window = int(np.ceil(len(motif) / 2))
    mot_shuf = np.array([12, 0, 1, 11, 10, 3, 2, 8, 9, 4, 5, 7, 6])

    
    for ind in range(num_seqs):
        try:
            chrom, start, end, gc = dataframe.iloc[ind][["chrom", "start", "end", "GC"]]
        except:
            chrom, start, end, strand = site_df.iloc[ind][["chrom", "start", "end", "strand"]]
            print('The dataframe used doesnot have GC content')
            
        genome_open = pysam.Fastafile(genome_fasta)
        seq = genome_open.fetch(chrom, start, end).upper()
        seq_1hot = dna_io.dna_1hot(seq)

        t0 = time.time()
        num_iters = 0
        while num_iters < max_iters:
            print("ind", ind, ", iter ", num_iters, ", for", chrom, start, end)
            # print(len(flat_seqs))

            seq_1hot_batch = []
            for i in range(batch_size):
                seq_1hot_mut = permute_seq_k(seq_1hot, k=shuffle_k)
                if masking == True:
                    s = scan_motif(seq_1hot_mut, motif)
                    for i in np.where(s > ctcf_thresh)[0]:
                        # seq_1hot_mut[i-motif_window:i+motif_window] = permute_seq_k(seq_1hot_mut[i-motif_window:i+motif_window], k=2)
                        seq_1hot_mut[
                            i - motif_window + 1 : i + motif_window
                        ] = seq_1hot_mut[i - motif_window + 1 : i + motif_window][mot_shuf]
                    seq_1hot_batch.append(seq_1hot_mut)
                else:
                    seq_1hot_batch.append(seq_1hot_mut)
            seq_1hot_batch = np.array(seq_1hot_batch)

            pred = seqnn_model.predict(seq_1hot_batch, batch_size=batch_size)
            scores = np.sum(pred**2, axis=-1).sum(axis=-1)
            scores_pixelwise = np.max(pred**2, axis=-1).max(axis=-1)
            t1 = time.time()
            
            if np.any([
                    (np.min(scores) < scores_thresh)
                    , (np.min(scores_pixelwise) < scores_pixelwise_thresh)]
            ):
                
                best_ind = np.argmin(scores_pixelwise)
                best_seq = seq_1hot_batch[best_ind]
                best_pred = pred[best_ind]
                best_score, best_score_pixelwise = (
                    scores[best_ind],
                    scores_pixelwise[best_ind],
                )
                num_iters = max_iters
                print(
                    "success: best seq, thresh",
                    np.min(scores),
                    " pixelwise",
                    np.min(scores_pixelwise),
                    "time",
                    t1 - t0
                )

            else:
                best_ind = np.argmin(scores_pixelwise)
                best_seq = seq_1hot_batch[best_ind]
                best_pred = pred[best_ind]
                best_score, best_score_pixelwise = (
                    scores[best_ind],
                    scores_pixelwise[best_ind],
                )
                print(
                    "trying: best seq, thresh",
                    np.min(scores),
                    " pixelwise",
                    np.min(scores_pixelwise),
                )

            num_iters += 1
            if num_iters >= max_iters:
                print(f"max iters exceeded, final time {t1 - t0}")
                
                if gc :
                    flat_seqs.append([
                    best_seq,
                    best_pred,
                    best_score,
                    best_score_pixelwise,
                    t1 - t0,
                    gc
                ])
                else:
                    flat_seqs.append([
                    best_seq,
                    best_pred,
                    best_score,
                    best_score_pixelwise,
                    t1 - t0
                ])
                # raise ValueError('cannot generate flat sequence for', chrom, start, end)

    return flat_seqs


def custom_calculate_scores(    seqnn_model,
                                genome_fasta,
                                seq_length,
                                dataframe,
                                max_iters=1,
                                batch_size=6,
                                shuffle_k=8,
                                ctcf_thresh=8,
                                scores_thresh=5500,
                                scores_pixelwise_thresh=0.04,
                                success_scores = 0,
                                masking=False,
                            ):
    """
    This function creates flat sequences
    """
    scores_set = []
    num_seqs = dataframe.shape[0]

    mot = ">CCAsyAGrkGGCr\n0.0000\t1.0000\t0.0000\t0.0000\n0.0000\t1.0000\t0.0000\t0.0000\n1.0000\t0.0000\t0.0000\t0.0000\n0.0000\t0.5000\t0.5000\t0.0000\n0.0000\t0.5000\t0.0000\t0.5000\n1.0000\t0.0000\t0.0000\t0.0000\n0.0000\t0.0000\t1.0000\t0.0000\n0.5000\t0.0000\t0.5000\t0.0000\n0.0000\t0.0000\t0.5000\t0.5000\n0.0000\t0.0000\t1.0000\t0.0000\n0.0000\t0.0000\t1.0000\t0.0000\n0.0000\t1.0000\t0.0000\t0.0000\n0.5000\t0.0000\t0.5000\t0.0000"
    motif = pd.read_csv(StringIO(mot), sep="\t", header=0, names=["A", "C", "G", "T"]).values
    motif_window = int(np.ceil(len(motif) / 2))
    mot_shuf = np.array([12, 0, 1, 11, 10, 3, 2, 8, 9, 4, 5, 7, 6, 13])

    
    for ind in range(num_seqs):
        chrom, start, end, gc = dataframe.iloc[ind][["chrom", "start", "end", "GC"]]
        genome_open = pysam.Fastafile(genome_fasta)
        seq = genome_open.fetch(chrom, start, end).upper()
        seq_1hot = dna_io.dna_1hot(seq)

        num_iters = 0
        while num_iters < max_iters:
            print("ind",ind,", iter ",num_iters,",k ",shuffle_k,", for", chrom, start, end,)
            seq_1hot_batch = []
            for i in range(batch_size):
                seq_1hot_mut = permute_seq_k(seq_1hot, k=shuffle_k)
                if masking == True:
                    s = scan_motif(seq_1hot_mut, motif)
                    for i in np.where(s > ctcf_thresh)[0]:
                        if len(seq_1hot_mut[i-motif_window:i+motif_window]) == len(mot_shuf):
                            seq_1hot_mut[i-motif_window:i+motif_window] = permute_seq_k(seq_1hot_mut[i-motif_window:i+motif_window], k=2)
                        
                        # seq_1hot_mut[i-motif_window:i+motif_window] = seq_1hot_mut[i-motif_window:i+motif_window][mot_shuf]
                    seq_1hot_batch.append(seq_1hot_mut)
                else:    
                    seq_1hot_batch.append(seq_1hot_mut)

            seq_1hot_batch = np.array(seq_1hot_batch)
            pred = seqnn_model.predict(seq_1hot_batch, batch_size=batch_size)
            scores = np.sum(pred**2, axis=-1).sum(axis=-1)
            scores_pixelwise = np.max(pred**2, axis=-1).max(axis=-1)
            

            if success_scores == 1:
                if np.any([
                    (np.min(scores) < scores_thresh)
                    , (np.min(scores_pixelwise) < scores_pixelwise_thresh)]
                    ):
                    scores_set += [scores]
                    best_ind = np.argmin(scores_pixelwise)
                    best_seq = seq_1hot_batch[best_ind]
                    best_pred = pred[best_ind]
                    best_score, best_score_pixelwise = (
                        scores[best_ind],
                        scores_pixelwise[best_ind],
                    )

                    print(
                        "*** success: best seq, thresh",
                        np.min(scores),
                        " pixelwise",
                        np.min(scores_pixelwise),
                        "***",
                    )
                else:
                    print(
                    "trying: best seq, thresh",
                    np.min(scores),
                    " pixelwise",
                    np.min(scores_pixelwise),
                    )
            else:
                    scores_set += [scores]
                
            num_iters += 1
            if num_iters >= max_iters:
                print("max iters exceeded")

    return scores_set
=======
# dataframe utils
def split_df_equally(df, num_chunks, chunk_idx):
    
    df_len = len(df)    # but indices are 0 -> 198

    chunks_bounds = np.linspace(
        0, df_len, num_chunks + 1, dtype="int"
    )

    df_chunk = df.loc[
        chunks_bounds[chunk_idx] : (chunks_bounds[chunk_idx + 1]-1), :
    ]
    
    return df_chunk
>>>>>>> main
